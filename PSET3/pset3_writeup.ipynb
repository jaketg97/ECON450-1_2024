{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3\n",
    "Jacob Toner Gosselin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "### 1.\n",
    "\n",
    "We are given Cobb-Douglas production, i.e. $F(k_t, l_t, m_t) = K_t^{\\alpha_K}L_t^{\\alpha_L}M_t^{\\alpha_M}$, and an AR(1) process for productivity $\\omega_t = \\delta_0 + \\delta_1\\omega_{t-1} + \\eta_t$.\n",
    "\n",
    "Equation (9) of GNR is $E[y_{jt} | \\Gamma_{jt}] = E[f(k_{jt}, l_{jt}, m_{jt}) | \\Gamma_{jt}] + h(\\phi(k_{jt-1}, l_{jt-1},m_{jt-1}) + d_{t-1} - f(k_{jt-1}, l_{jt-1}, m_{jt-1})) $\n",
    "\n",
    "Where $d_t = ln(\\rho_t/P_t) - ln(\\Sigma)$, $\\Sigma = E[e^{\\epsilon_{jt}}|I_{jt}]$, $\\phi(k_{jt}, l_{jt}, m_{jt}) = f(k_{jt}, l_{jt}, m_{jt}) + M^{-1}(k_{jt}, l_{jt}, m_{jt})$, and $h(\\omega_{jt-1}) = E[\\omega_{jt}| \\omega_{jt-1}] = \\delta_0 + \\delta_1\\omega_{jt-1}$\n",
    "\n",
    "Plugging in our givens, we can write Equation (9) as...\n",
    "\n",
    "$E[y_{jt} | \\Gamma_{jt}] = \\alpha_K k_{jt} + \\alpha_L l_{jt} + \\alpha_M E[m_{jt}| \\Gamma_{jt}] + h(M^{-1}(k_{jt-1}, l_{jt-1}, m_{jt-1}) + d_{t-1})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. \n",
    "\n",
    "For conciseness, define $F_{jt} = K_{jt}^{\\alpha_K}L_{jt}^{\\alpha_L}M_{jt}^{\\alpha_M}$\n",
    "\n",
    "The firms problem is $max[P_{t}F_{jt}e^{\\omega_{jt}}\\Sigma - M_{jt}\\rho_t]$ with respect to $M_{jt}$\n",
    "\n",
    "The corresponding first-order-condition is $P_{t} \\alpha_M \\frac{F_t}{M_{jt}} e^{\\omega_{jt}} \\Sigma = \\rho_t$\n",
    "\n",
    "Solving for M, you find $M_{jt} = (\\frac{P_t}{\\rho_t} K_{jt}^{\\alpha_K} L_{jt}^{\\alpha_L} \\alpha_M e^{\\omega_{jt}} \\Sigma)^{\\frac{1}{1-\\alpha_M}}$\n",
    "\n",
    "Solving for $m=ln(M)$, we get $m_{jt} = \\frac{1}{1-\\alpha_M} (\\alpha_K k_{jt} + \\alpha_L l_{jt} + \\omega_{jt} + ln(\\alpha_M) - d_t) $\n",
    "\n",
    "So $E[m_{jt} | \\Gamma_{jt}] = E[\\frac{1}{1-\\alpha_M} (\\alpha_K k_{jt} + \\alpha_L l_{jt} + \\omega_{jt} + ln(\\alpha_M) - d_t) |  \\Gamma_{jt}] = \\frac{1}{1-\\alpha_M} (\\alpha_K k_{jt} + \\alpha_L l_{jt} + E[\\omega_{jt} | \\Gamma_{jt}]  + ln(\\alpha_M) - d_t) = \\frac{1}{1-\\alpha_M} (\\alpha_K k_{jt} + \\alpha_L l_{jt} + h(\\omega_{jt-1}) + ln(\\alpha_M) - d_t) $\n",
    "\n",
    "Plugging in, we have \n",
    "\n",
    "$E[y_{jt} | \\Gamma_{jt}] = \\alpha_K k_{jt} + \\alpha_L l_{jt} + \\frac{\\alpha_M}{1-\\alpha_M} (\\alpha_K k_{jt} + \\alpha_L l_{jt} + h(\\omega_{jt-1}) + ln(\\alpha_M) - d_t)) + h(M^{-1}(k_{jt-1}, l_{jt-1}, m_{jt-1}) + d_{t-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "$\\phi_{t-1} = \\phi(k_{jt-1}, l_{jt-1}, m_{jt-1}) = f(k_{jt-1}, l_{jt-1}, m_{jt-1}) + M^{-1}(k_{jt-1}, l_{jt-1}, m_{jt-1})$\n",
    "\n",
    "From above, we can see $ \\omega_{jt} = (1-\\alpha_M) m_{jt} - \\alpha_K k_{jt} - \\alpha_L l_{jt} - ln(\\alpha_M) + d_t $\n",
    "\n",
    "GNR defines the inverted proxy equation as $\\omega_{jt} = M^{-1}(k_{jt}, l_{jt}, m_{jt}) + d_t$, so $M^{-1}(k_{jt}, l_{jt}, m_{jt}) = (1-\\alpha_M) m_{jt} - \\alpha_K k_{jt} - \\alpha_L l_{jt} - ln(\\alpha_M)$\n",
    "\n",
    "Plugging in, $\\phi_{t-1} = m_{jt-1} - ln(\\alpha_M)$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. \n",
    "\n",
    "Putting everything together, we have...\n",
    "\n",
    "$ y_{jt} = $\n",
    "\n",
    "$E[y_{jt} | \\Gamma_{jt}] + \\epsilon_{jt} + \\eta_{jt} =$\n",
    "\n",
    "$\\alpha_K k_{jt} + \\alpha_L l_{jt} + \\frac{\\alpha_M}{1-\\alpha_M} (\\alpha_K k_{jt} + \\alpha_L l_{jt} + h(\\omega_{jt-1}) + ln(\\alpha_M) - d_t)) + h( (1-\\alpha_M) m_{jt-1} - \\alpha_K k_{jt-1} - \\alpha_L l_{jt-1} - ln(\\alpha_M) + d_{t-1}) + \\epsilon_{jt} + \\eta_{jt}$\n",
    "\n",
    "Our coefficients aren't identified unless there's sufficient time-series variation in prices ($d_t$). We're basically re-showing Theorem 1 of GNR here; note the $\\omega_{jt-1}$ lingering in the term for $E[m_{jt} | \\Gamma_{jt}]$. We've already conditioned on $k_{jt}, l_{jt}, d_{t-1}, k_{jt-1}, l_{jt-1}, m_{jt-1}$; all we have left is $d_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. \n",
    "\n",
    "Our \"share\" term is $ln(\\frac{\\rho_t M_{jt}}{P_t Y_{jt}})$\n",
    "\n",
    "From FOCs, $ln(\\alpha_M) = ln(\\frac{\\rho_t M_{jt}}{P_t Y_{jt}})$ (it's Cobb-Douglas, so share = coefficient)\n",
    "\n",
    "Observe $ln(\\alpha_M) = ln (\\frac{d}{dm_{t}} f_t)$; we can integrate this to get get our production function $f_t$ to a constant (which we can then estimate via the share equation above). \n",
    "\n",
    "One parameter ($\\alpha_M$) is identified, but the others aren't (this is non-parametric identification). We get productivity ($\\omega_{jt}$) by subtracting this estimate from $y_{jt}$, and that gives us the productivity process as in GNR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I completed questions 2.1, 2.2, 2.3, 2.4, 2.5.1 (Arellano and Bond estimator) and 2.5.2 (Blundell and Bond estimator) in Stata. The results are all listed in tables at the end of this write-up (titled, appropriately, Appendix: Stata results). For 2.1 and 2.2, it wasn't clear to me how you wanted these sample statistics reported (there are 18 industries, so reporting number of firms per industry-year, and the count of firms missing one 5 variables per industry-year, would be a **massive** table). I did calculate all these values, but simply logged the results (and am attaching the corresponding log file). Apologies, I know this is not best practice, but this seemed a clear case of diminishing marginal return on effort. From 2.3 on, I used **Industry 13**. For 2.3, I didn't do the first-difference or long-difference regressions, as (I believe) they would be meaningless without the balanced panel. \n",
    "\n",
    "For questions 2.5.3-2.5.6, i.e. OP, LP, ACF, and GNR estimation, all work was done in Python. I will run through how I estimated each below, and report results. A couple general notes.\n",
    "1. All standard errors were calculated via Bootstrap, with 100 reps. I don't have standard errors for my estimates for GNR; when I bootstrap sample, it takes incredibly long for my GNR estimates to converge (~10 minutes). I presume this is because the lost information when sampling with replacement makes the optimization problems slower; I don't know how to speed it up, as I am already feeding it the gradient via auto-differentiation. \n",
    "2. For OP and ACF, I use value-added output, while for LP and GNR, I use gross output; when I use \"y\" in my explanations, it corresponds appropriately.\n",
    "3. Often in my explanations and my code I define $\\omega_t = \\phi_t - \\beta_K k_t (- \\beta_L l_t) (- \\beta_M m_t)$. I know this is technically $\\omega_t + \\beta_0$, but it doesn't really matter, since everything cancels out when we get the $\\xi$ and $\\epsilon$ we build our conditions on (the Stata codes for LP do it this way too, I believe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Olley and Pakes\n",
    "\n",
    "I do OP in two stages. In stage 1, I estimate $y = \\beta_L l + \\phi(k, i)$ via OLS, where $\\phi(k,i)$ is a third order polynomial. In stage 2, I form an objective function $V(\\beta_{K})$. Given a $\\beta_{K}$, I calculate $\\omega_{t} = \\phi_{t} - \\beta_{k}k_t$, and $\\omega_{t-1}$ correspondingly. I also estimate a survival probability $P_t$ logistically, using a third order polynomial of k and i. I then estimate $E[\\omega_{t}|\\omega_{t-1}, P_{t-1}]$ using OLS of $\\omega_{t}$ on a third order polynomial of $\\omega_{t-1}$ and $P_{t-1}$. I define $\\xi + \\epsilon  = y - \\beta_{K} k - \\beta_{L} l - E[\\omega_{t}|\\omega_{t-1}, P_{t-1}]$, and use moments built off $E[\\xi + \\epsilon] = 0$. In practice, I actually minimize the sum of squared residuals, i.e. $(\\xi + \\epsilon)^2$, as we're only estimating one parameter in the second step (so we don't need multiple moments). My results are below (estimates of $\\beta_K, \\beta_L$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          estimates    standard errors\n",
      "beta_k     0.331836          0.0831012\n",
      "beta_l     0.791161          0.01525\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tabulate import tabulate\n",
    "from PSET3_functions.misc import gen_data, boot \n",
    "from PSET3_functions.op_est import OP_estimation\n",
    "\n",
    "q2_data = pd.read_stata(\"data/PS3_data.dta\")\n",
    "industry13_unbalanced = q2_data[q2_data[\"X16\"] == 1].copy()\n",
    "main_data = gen_data(industry13_unbalanced)\n",
    "\n",
    "opdat = main_data[main_data[\"i\"] != 0].copy()\n",
    "op_est = OP_estimation(opdat)\n",
    "op_bootstrap = boot(opdat, OP_estimation, 10)\n",
    "op_bootstrap = pd.DataFrame(op_bootstrap, columns = [\"beta_k\", \"beta_l\"])\n",
    "\n",
    "beta_k, beta_l = op_est \n",
    "beta_k_se, beta_l_se = np.std(op_bootstrap, axis = 0)\n",
    "results = [[\"beta_k\", beta_k, beta_k_se], \n",
    "           [\"beta_l\", beta_l, beta_l_se]]\n",
    "headers = [\"estimates\", \"standard errors\"]\n",
    "print(tabulate(results, headers, tablefmt=\"plain\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levinsohn and Petrin\n",
    "\n",
    "I do LP in two stages. In stage 1, I estimate $y = \\beta_L l + \\phi(k, m)$ via OLS, where $\\phi(k,m)$ is a third order polynomial. In stage 2, I form an objective function $V(\\beta_{K}, \\beta_{M})$. Given a pair of parameters, I calculate $\\omega_{t} = \\phi_{t} - \\beta_{K}k_t - \\beta_{M}m_t$, and $\\omega_{t-1}$ correspondingly. I then estimate $E[\\omega_{t}|\\omega_{t-1}]$ using OLS of $\\omega_{t}$ on a third order polynomial of $\\omega_{t-1}$ and $P_{t-1}$. I define $\\xi + \\epsilon  = y - \\beta_{K} k - \\beta_{L} l - \\beta_{M} m - E[\\omega_{t}|\\omega_{t-1}]$, and use moments built off $E[\\xi + \\epsilon | Z_t] = 0$. In practice, I minimize with moments of $Z_t = (k_t, m_{t-1}, l_{t-1}, m_{t-2}, k_{t-1})$; this is identical to Petrin et. al., 2004 (i.e the Stata command). **I use autodifferentiation via Google JAX and minimize using the BFGS algorithm**. My results are below (estimates of $\\beta_K, \\beta_M, \\beta_L$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          estimates    standard errors\n",
      "beta_k     1.03689          0.18372\n",
      "beta_m     0.970814         0.111606\n",
      "beta_l     0.266514         0.00857457\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tabulate import tabulate\n",
    "from PSET3_functions.misc import gen_data, boot \n",
    "from PSET3_functions.lp_est import LP_estimation, LP_firststage\n",
    "import os\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "q2_data = pd.read_stata(\"data/PS3_data.dta\")\n",
    "industry13_unbalanced = q2_data[q2_data[\"X16\"] == 1].copy()\n",
    "main_data = gen_data(industry13_unbalanced)\n",
    "lpdat = main_data[main_data[\"m\"] != 0].copy() #dropping observations with no materials data\n",
    "\n",
    "with open(os.devnull, 'w') as f, redirect_stdout(f):\n",
    "    lp_est = LP_estimation(lpdat)\n",
    "    lp_bootstrap = boot(lpdat, LP_estimation, 10)\n",
    "\n",
    "lp_bootstrap = pd.DataFrame(lp_bootstrap, columns = [\"beta_k\", \"beta_m\", \"beta_l\"])\n",
    "beta_k, beta_m, beta_l = lp_est \n",
    "beta_k_se, beta_m_se, beta_l_se = np.std(lp_bootstrap, axis = 0)\n",
    "results = [[\"beta_k\", beta_k, beta_k_se], \n",
    "           [\"beta_m\", beta_m, beta_m_se],\n",
    "           [\"beta_l\", beta_l, beta_l_se]]\n",
    "headers = [\"estimates\", \"standard errors\"]\n",
    "print(tabulate(results, headers, tablefmt=\"plain\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ackerberg, Caves and Frazier \n",
    "\n",
    "I do ACF in two stages. In stage 1, I estimate $y = \\phi(l, k, m)$ via OLS, where $\\phi(l, k,m)$ is a third order polynomial. In stage 2, I form an objective function $V(\\beta_{K}, \\beta_{L})$. Given a pair of parameters, I calculate $\\omega_{t} = \\phi_{t} - \\beta_{K}k_t - \\beta_{L}l_t$, and $\\omega_{t-1}$ correspondingly. I then estimate $E[\\omega_{t}|\\omega_{t-1}]$ using OLS of $\\omega_{t}$ on a third order polynomial of $\\omega_{t-1}$. I define $\\xi  = \\omega_{t} - E[\\omega_{t}|\\omega_{t-1}]$, and use moments built off $E[\\xi | Z_t] = 0$. In practice, I minimize with moments of $Z_t = (k_t, l_{t-1})$. I weigh moments using the identity matrix. **I use autodifferentiation via Google JAX and minimize using the BFGS algorithm**. My results are below (estimates of $\\beta_K$, $\\beta_L$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          estimates    standard errors\n",
      "beta_k     0.126929           0.275205\n",
      "beta_l     1.16189            0.124654\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from PSET3_functions.misc import gen_data, boot \n",
    "from PSET3_functions.acf_est import ACF_estimation\n",
    "import os\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "q2_data = pd.read_stata(\"data/PS3_data.dta\")\n",
    "industry13_unbalanced = q2_data[q2_data[\"X16\"] == 1].copy()\n",
    "main_data = gen_data(industry13_unbalanced)\n",
    "acfdat = main_data[main_data[\"m\"] != 0].copy() #dropping observations with no materials data\n",
    "\n",
    "with open(os.devnull, 'w') as f, redirect_stdout(f):\n",
    "    acf_est = ACF_estimation(acfdat)\n",
    "    acf_bootstrap = boot(acfdat, ACF_estimation, 10)\n",
    "\n",
    "acf_bootstrap = pd.DataFrame(acf_bootstrap, columns = [\"beta_k\", \"beta_l\"])\n",
    "\n",
    "beta_k, beta_l = acf_est \n",
    "beta_k_se, beta_l_se = np.std(acf_bootstrap, axis = 0)\n",
    "results = [[\"beta_k\", beta_k, beta_k_se], \n",
    "           [\"beta_l\", beta_l, beta_l_se]]\n",
    "headers = [\"estimates\", \"standard errors\"]\n",
    "print(tabulate(results, headers, tablefmt=\"plain\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gandhi, Navarro and Rivers\n",
    "\n",
    "I do GNR in two stages. In stage 1, I recover a non-parametric estimate of $df(k, l , m)/dm$, i.e. the flexible input elasticity, by \n",
    "1. minimizing $\\sum (s_{jt} - ln(\\phi(k,l,m))^2)$, where $\\phi(k,l,m)$ is a third order polynomial, with respect to the parameters $\\gamma$ attached to that polynomial.\n",
    "2. divide $\\gamma$ by $\\mathbb{E} = 1/JT \\sum e^{\\epsilon_jt}$, i.e. the expected value of e raised to the residuals from the minimization above\n",
    "3. dot product these new $\\gamma'$ with their corresponding term in third order polynomial\n",
    "\n",
    "In stage 2, I integrate this estimate directly (it's a polynomial, so analytic), which gives us the production function up to a constant C (define as $\\mathbb{D}$). I estimate this C nonparametrically, as $C(k,l)$, using another third order polynomial of k and l. I define an objective function $V(\\alpha)$, where $\\alpha$ is the parameters corresponding to $C(k,l)$. I define $\\mathbb{Y} = y - \\epsilon - \\mathbb{D} = \\omega - C$. Since $\\omega = \\mathbb{Y} + C$, I define $\\omega_{t}$ and $\\omega_{t-1}$ correspondingly, and then estimate $E[\\omega_{t}|\\omega_{t-1}]$ by a third order polynomial regression of $\\omega_{t}$ on $\\omega_{t-1}$. I define $\\xi = \\omega_{t} - E[\\omega_{t}|\\omega_{t-1}]$, and use moments built of $E[\\xi|Z]=0$. In practice, I use all terms in the third order polynomial of $C(k,l)$ as moments. I weigh using the identity matrix. **I use autodifferentiation via Google JAX and minimize using the BFGS algorithm**. My results are below (resulting mean values of $df(k, l , m)/dm$, and mean values of $\\omega_{t}$). *For the record, I know the $\\omega$ values are ridiculous; I think this is because my polynomial estimate is very sensitive. The values of the third order terms are massive, so small imprecisions during optimization throw things off in a major way. If I had more time, I'd do all the non-parametric steps with kernels instead.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            estimates\n",
      "E[df/dm]      1.66219\n",
      "E[omega]     11.3425\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from PSET3_functions.misc import gen_data\n",
    "from PSET3_functions.gnr_est import GNR_estimation\n",
    "\n",
    "q2_data = pd.read_stata(\"data/PS3_data.dta\")\n",
    "industry13_unbalanced = q2_data[q2_data[\"X16\"] == 1].copy()\n",
    "main_data = gen_data(industry13_unbalanced)\n",
    "gnrdat = main_data[main_data[\"m\"] != 0].copy() #dropping observations with no materials data\n",
    "\n",
    "import os\n",
    "from contextlib import redirect_stdout\n",
    "with open(os.devnull, 'w') as f, redirect_stdout(f):\n",
    "    gnr_est = GNR_estimation(gnrdat)\n",
    "\n",
    "dm, omega = gnr_est \n",
    "results = [[\"E[df/dm]\", dm], \n",
    "           [\"E[omega]\", omega]]\n",
    "headers = [\"estimates\"]\n",
    "print(tabulate(results, headers, tablefmt=\"plain\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MacM3Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
